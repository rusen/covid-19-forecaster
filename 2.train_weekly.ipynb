{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import shutil\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import *\n",
    "import keras\n",
    "import pickle\n",
    "\n",
    "import requests\n",
    "from google.cloud import storage\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "lookahead = 8 # Lookahead weeks. Set to 8 by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important dates\n",
    "begin_date_str = '1/27/20' # Make sure it's a monday, and the day before exists in the data\n",
    "begin_date = datetime.datetime.strptime(begin_date_str, \"%m/%d/%y\").date()\n",
    "print(f'Begin date : {begin_date_str}')\n",
    "\n",
    "today = datetime.date.today()\n",
    "today_date = f'{today.month}/{today.day}/{today.year-2000}'\n",
    "print(f'Date today : {today_date}')\n",
    "\n",
    "end_date = today - datetime.timedelta(today.weekday()+1)\n",
    "end_date_str = f'{end_date.month}/{end_date.day}/{end_date.year-2000}'\n",
    "print(f'End of last week : {end_date_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output folder\n",
    "week_begin = today - datetime.timedelta(today.weekday())\n",
    "week_end = week_begin + datetime.timedelta(days=6)\n",
    "cur_week_name = f'{week_begin.month}-{week_begin.day}-{week_begin.year-2000}-to-{week_end.month}-{week_end.day}-{week_end.year-2000}'\n",
    "preds_folder = f'./preds/weekly/{cur_week_name}'\n",
    "output_folder = f'./output/weekly/{cur_week_name}'\n",
    "print(f'Current identifier: *{cur_week_name}*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{output_folder}/country_daily.csv\")\n",
    "\n",
    "# Write individual predictions to file\n",
    "with open(f\"{output_folder}/cols.pkl\",\"rb\") as f:\n",
    "    cols = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cols = cols['base_cols']\n",
    "day_cols = cols['day_cols']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add city data \n",
    "region_data = pd.read_csv(f'{output_folder}/all_region_df.csv', index_col=0)\n",
    "region_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_data.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df, region_data]).reset_index(drop=True)\n",
    "len(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekly or daily preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_type = 'weekly'\n",
    "test_data_size = 2 # number of samples allocated for test and val sets (separately). \n",
    "                   # Set to 14 if days, 2 if weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select = deepcopy(df_all[base_cols])\n",
    "\n",
    "if pred_type == 'weekly':\n",
    "    select_cols = []\n",
    "    for i in range(len(day_cols) // 7):\n",
    "        df_select[day_cols[i*7]] = df_all[day_cols[(i*7):((i*7)+7)]].sum(axis=1)\n",
    "        select_cols.append(day_cols[i*7])\n",
    "elif pred_type == 'daily':\n",
    "    select_cols = day_cols\n",
    "    for day in day_cols:\n",
    "        df_select[day] = df_all[day]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cols\n",
    "train_cols = select_cols[:(2 * -test_data_size)]\n",
    "val_cols = select_cols[(-2 * test_data_size):-test_data_size]\n",
    "test_cols = select_cols[-test_data_size:]\n",
    "print('train cols:', train_cols)\n",
    "print('val cols:', val_cols)\n",
    "print('test cols:', test_cols)\n",
    "\n",
    "# Get data\n",
    "train_data = df_select[select_cols].iloc[:, :(2 * -test_data_size)]\n",
    "val_data = df_select[select_cols].iloc[:, (-2 * test_data_size):-test_data_size]\n",
    "test_data = df_select[select_cols].iloc[:, -test_data_size:]\n",
    "print(train_data.shape, val_data.shape, test_data.shape)\n",
    "# weekly_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_val = np.std(train_data.values)\n",
    "std_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data / std_val\n",
    "val_data = val_data / std_val\n",
    "test_data = test_data / std_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get UK Index\n",
    "uk_idx = np.nonzero((df_select['Country'] == 'United Kingdom').values)\n",
    "uk_idx = uk_idx[0][0]\n",
    "uk_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the UK\n",
    "plot_data = df_select.loc[uk_idx, train_cols].values\n",
    "plt.plot([x for x in range(len(train_cols))], 100000 * plot_data)\n",
    "plt.ylabel('weekly cases per 100k')\n",
    "plt.xlabel('week')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find hyperparameters for a good predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, layer_count, units, dropout, training=None):\n",
    "    # Layer count check\n",
    "    if layer_count < 2:\n",
    "        return None\n",
    "    \n",
    "    # Add the first layer\n",
    "    inputs = keras.Input(shape=(input_shape, 1))\n",
    "    x = LSTM(units=units, return_sequences=True)(inputs)\n",
    "    \n",
    "    # Add further layers\n",
    "    for layer in range(1, layer_count):\n",
    "        # Output layer: set units to 0 and don't return sequences\n",
    "        if layer == (layer_count-1):\n",
    "            return_sequences = False\n",
    "        else:\n",
    "            return_sequences = True\n",
    "\n",
    "        # Add LSTM layer\n",
    "        x = LSTM(units=units, return_sequences=return_sequences)(x)\n",
    "        x = Dropout(dropout)(x, training=training)\n",
    "\n",
    "    # Adding the output layer\n",
    "    outputs = Dense(1, activation='relu')(x)\n",
    "\n",
    "    # Compiling the model\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, val_data, input_steps):\n",
    "    batch_size = 128\n",
    "    num_train_cols = train_data.shape[1]\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Create training data\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for i in range(len(train_data)):\n",
    "        for j in range(input_steps, num_train_cols):\n",
    "            X_train.append(train_data.iloc[i, (j-input_steps):j])\n",
    "            y_train.append(train_data.iloc[i, j])\n",
    "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "    # Remove 80% of the data that is completely zero\n",
    "    zero_samples = np.all(X_train == 0, axis=1).squeeze() & (y_train == 0)\n",
    "    valid_samples = ~(zero_samples & (np.random.rand(len(zero_samples)) > 0.4))\n",
    "    X_train = X_train[valid_samples, :]\n",
    "    y_train = y_train[valid_samples]\n",
    "    print('Train', X_train.shape, y_train.shape)\n",
    "    \n",
    "    # Create validation data\n",
    "    if val_data is not None:\n",
    "        X_val = []\n",
    "        y_val = []\n",
    "        combined_data = pd.concat([train_data, val_data], axis=1)\n",
    "        print(combined_data.shape)\n",
    "\n",
    "        for i in range(len(val_data)):\n",
    "            for j in range(num_train_cols, combined_data.shape[1]):\n",
    "                X_val.append(combined_data.iloc[i, (j-input_steps):j])\n",
    "                y_val.append(combined_data.iloc[i, j])\n",
    "        X_val, y_val = np.array(X_val), np.array(y_val)\n",
    "        X_val = np.reshape(X_val, (X_val.shape[0], X_val.shape[1], 1))\n",
    "        val_data_agg = (X_val, y_val)\n",
    "        print('Val', X_val.shape, y_val.shape)    \n",
    "        history = model.fit(X_train, y_train, validation_data=val_data_agg, epochs = 150, verbose=2, batch_size = batch_size, callbacks=[])\n",
    "        history = model.fit(X_train, y_train, validation_data=val_data_agg, epochs = 150, verbose=2, batch_size = batch_size, callbacks=[callback])\n",
    "    else: \n",
    "        history = model.fit(X_train, y_train, validation_split=0.1, epochs = 150, verbose=2, batch_size = batch_size, callbacks=[])\n",
    "        history = model.fit(X_train, y_train, validation_split=0.1, epochs = 150, verbose=2, batch_size = batch_size, callbacks=[callback])\n",
    "\n",
    "    # Fitting the LSTM to the training set\n",
    "    return min(history.history['val_loss']), history\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform a grid search for parameters\n",
    "# stats = {}\n",
    "# for input_steps in [15, 10, 8, 5]:\n",
    "#     for units in [10, 20, 50, 100]:\n",
    "#         for dropout in [0.2, 0.3, 0.5]:\n",
    "#             for layers in [3, 5, 8]:\n",
    "#                 print('Training ', input_steps, units, dropout, layers)\n",
    "#                 model = create_model(input_steps, layers, units, dropout)\n",
    "#                 min_val_loss = train_model(model, train_data, val_data, input_steps)\n",
    "#                 stats[input_steps, units, dropout, layers] = min_val_loss\n",
    "#                 print('Min val loss for ', input_steps, units, dropout, layers, ':', min_val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outcome: best model = create_model(5, 3, 50, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_steps = 8\n",
    "layer_count = 3\n",
    "unit_count = 50\n",
    "dropout = 0.4\n",
    "best_model_path = f\"{output_folder}/best_{input_steps}_{layer_count}_{unit_count}_{dropout}.h5\"\n",
    "model = create_model(input_steps, layer_count, unit_count, dropout)\n",
    "\n",
    "# Train model\n",
    "min_val_loss, history = train_model(model, pd.concat([train_data, val_data, test_data], axis=1), None, input_steps)\n",
    "model.save_weights(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = [x for x in range(len(history.history['val_loss']))]\n",
    "plt.plot(x_vals, history.history['val_loss'])\n",
    "plt.plot(x_vals, history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data, determine lookahead\n",
    "combined_data = pd.concat([train_data, val_data, test_data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "pred_vars = {}\n",
    "\n",
    "pred_vars['std_val'] = std_val\n",
    "pred_vars['test_data_size'] = test_data_size\n",
    "pred_vars['pred_type'] = pred_type\n",
    "pred_vars['base_cols'] = base_cols\n",
    "pred_vars['combined_data'] = combined_data\n",
    "pred_vars['df_select'] = df_select\n",
    "pred_vars['input_steps'] = input_steps\n",
    "pred_vars['layer_count'] = layer_count\n",
    "pred_vars['unit_count'] = unit_count\n",
    "pred_vars['dropout'] = dropout\n",
    "pred_vars['best_model_path'] = f\"{output_folder}/best_{input_steps}_{layer_count}_{unit_count}_{dropout}.h5\"\n",
    "with open(f\"{output_folder}/pred_vars.pkl\",\"wb\") as f:\n",
    "    pickle.dump(pred_vars,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train with the best model\n",
    "# input_steps = 5\n",
    "# model = create_model(input_steps, 3, 50, 0.2)\n",
    "# min_val_loss = train_model(model, pd.concat([train_data], axis=1), None, input_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_lookahead = 4\n",
    "\n",
    "# all_preds = []\n",
    "# cur_data = train_data.values[:, -input_steps:]\n",
    "# # Get preds for all future days\n",
    "# for day in range(tmp_lookahead):\n",
    "#     tmp_data = np.reshape(cur_data, (cur_data.shape[0], cur_data.shape[1], 1))\n",
    "#     preds = model.predict(tmp_data)\n",
    "#     all_preds.append(preds)\n",
    "#     cur_data = np.concatenate([cur_data, preds], axis=1)[:, -input_steps:]\n",
    "# all_preds = np.concatenate(all_preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real_data = np.concatenate([train_data, all_preds], axis=1) * std_val\n",
    "# pred_data = np.concatenate([train_data, val_data, test_data], axis=1) * std_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_cols = 20\n",
    "# buffer = 0.2\n",
    "# ybuffer = 0.5\n",
    "# future_weeks = val_data.shape[1] + test_data.shape[1]\n",
    "# if os.path.exists(f'{output_folder}/sanity_check'):\n",
    "#     shutil.rmtree(f'{output_folder}/sanity_check')\n",
    "# os.makedirs(f'{output_folder}/sanity_check')\n",
    "# for idx in range(len(df_select)):\n",
    "#     country_name = df_select.loc[idx, 'Country']\n",
    "#     code_name = df_select.loc[idx, 'CCODE']\n",
    "#     data_to_show = np.maximum(0.0, 100000 * real_data[idx, -show_cols:])\n",
    "#     real_data_to_show = np.maximum(0.0, 100000 * pred_data[idx, -show_cols:])\n",
    "\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.title(f'Weekly Covid-19 data for {country_name}')\n",
    "#     plt.plot([x for x in range(len(data_to_show))], data_to_show, 'c--o')\n",
    "#     plt.plot([x for x in range(len(real_data_to_show))], real_data_to_show, 'b-o')\n",
    "#     plt.plot([0, show_cols-1], [20, 20], 'r--')\n",
    "#     plt.ylabel('weekly cases per 100k')\n",
    "#     plt.xlabel('week')\n",
    "#     plt.xlim([-buffer, (show_cols-1)+buffer])\n",
    "#     plt.gcf().subplots_adjust(bottom=0.3, left=0.1, right=0.96)\n",
    "#     plt.savefig(f'{output_folder}/sanity_check/{code_name}.jpg', dpi=200)\n",
    "#     plt.show()\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
