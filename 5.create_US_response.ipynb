{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook scrapes US travel restrictions and travel advice. \n",
    "Bugra Ozkan (bugraozkan@live.com) contributed to this codebase by scraping US categories and summaries from each country's website, and those parts are re-implemented in Python to include them in this codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import shutil\n",
    "import os\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import *\n",
    "import keras\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "\n",
    "import requests\n",
    "from google.cloud import storage\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the following environment variable is set! \n",
    "creds_path = os.environ['GOOGLE_APPLICATION_CREDENTIALS_PATH']\n",
    "gcs_client = storage.Client.from_service_account_json(creds_path)\n",
    "bucket = gcs_client.get_bucket('covid-19-forecaster-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_advice_url = \"https://www.cdc.gov/coronavirus/2019-ncov/travelers/map-and-travel-notices.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "begin_date_str = '1/27/20' # Make sure it's a monday, and the day before exists in the data\n",
    "begin_date = datetime.datetime.strptime(begin_date_str, \"%m/%d/%y\")\n",
    "\n",
    "today = datetime.date.today()\n",
    "today_date = f'{today.month}/{today.day}/{today.year-2000}'\n",
    "today_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output folder\n",
    "week_begin = today - datetime.timedelta(today.weekday())\n",
    "week_end = week_begin + datetime.timedelta(days=6)\n",
    "cur_week_name = f'{week_begin.month}-{week_begin.day}-{week_begin.year-2000}-to-{week_end.month}-{week_end.day}-{week_end.year-2000}'\n",
    "output_folder = f'./preds/weekly/{cur_week_name}'\n",
    "print(f'Current identifier: *{cur_week_name}*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv(f'./output/weekly/{cur_week_name}/df_final.csv', index_col=0)\n",
    "with open(f\"./output/weekly/{cur_week_name}/config.pkl\",\"rb\") as f:\n",
    "    config = pickle.load(f)\n",
    "with open(f\"./output/weekly/{cur_week_name}/all_preds.pkl\",\"rb\") as f:\n",
    "    pred_weeks_data = pickle.load(f)\n",
    "with open(f\"./output/weekly/{cur_week_name}/risk_preds.pkl\",\"rb\") as f:\n",
    "    risk_preds = pickle.load(f)\n",
    "\n",
    "# # Load US scrape\n",
    "# with open('./us-covid-scraper/output.json') as json_file: \n",
    "#     us_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get top-down US COUNTRY-CCODE MAPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_name_mapping = {\n",
    "    'Cabo Verde': 'Cape Verde', \n",
    "    'Congo, Rep.': 'Congo, Republic of the', \n",
    "    'Congo, Dem. Rep.': 'Democratic Republic of the Congo', \n",
    "    \"Cote d'Ivoire\": 'Ivory Coast ', \n",
    "    'Eswatini': 'Eswatini (Swaziland)', \n",
    "    'Israel': 'Israel, including the West Bank and Gaza', \n",
    "    'Korea, South': 'South Korea', \n",
    "    'Netherlands': 'Netherlands, The', \n",
    "    'Sao Tome and Principe': 'São Tomé and Príncipe', \n",
    "    'West Bank and Gaza': 'Israel, including the West Bank and Gaza', \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'Access-Control-Allow-Origin': '*',\n",
    "    'Access-Control-Allow-Methods': 'GET',\n",
    "    'Access-Control-Allow-Headers': 'Content-Type',\n",
    "    'Access-Control-Max-Age': '3600',\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [1,2,3]\n",
    "div_classes = {\n",
    "    1: ['ral3', 'ral4'],\n",
    "    2: ['ral2'],\n",
    "    3: ['ral1', 'ral5']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get(travel_advice_url, headers)\n",
    "soup = BeautifulSoup(req.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_scraped_data = {}\n",
    "for cls in classes:\n",
    "    for div in div_classes[cls]:\n",
    "        containers = soup.find_all('div', class_=div)\n",
    "        for container in containers:\n",
    "            for country_entry in container.find_all('li'):\n",
    "                try:\n",
    "                    country_name = country_entry.a.text\n",
    "                    country_url = country_entry.a['href']\n",
    "                except:\n",
    "                    country_name = country_entry.text\n",
    "                    country_url = \"N/A\"\n",
    "                country_scraped_data[country_name] = {'category': cls, 'advice_url': country_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_data = {}\n",
    "invalid_countries = []\n",
    "for i, row in df_final[df_final['is_country'] == 1].iterrows():\n",
    "    country_name = row['Country']\n",
    "    if country_name in country_name_mapping:\n",
    "        country_name = country_name_mapping[country_name]\n",
    "    if country_name in country_scraped_data:\n",
    "        us_data[row['CCODE']] = country_scraped_data[country_name]\n",
    "    else:\n",
    "        invalid_countries.append(row['Country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, get the list of countries which have specific travel advice.\n",
    "responses = {}\n",
    "headers = {\n",
    "    'Access-Control-Allow-Origin': '*',\n",
    "    'Access-Control-Allow-Methods': 'GET',\n",
    "    'Access-Control-Allow-Headers': 'Content-Type',\n",
    "    'Access-Control-Max-Age': '3600',\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0'\n",
    "    }\n",
    "for ccode in us_data:\n",
    "    new_url = us_data[ccode]['advice_url']\n",
    "    if new_url != \"N/A\":\n",
    "        try:\n",
    "            req = requests.get(new_url, headers, timeout = 4)\n",
    "            responses[ccode] = req\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary of the advice.\n",
    "summaries = {}\n",
    "for ccode in us_data:\n",
    "    us_data[ccode]['summary'] = ''\n",
    "    if ccode in responses:\n",
    "        req = responses[ccode]\n",
    "        soup = BeautifulSoup(req.content, 'html.parser')\n",
    "        start = soup.find('div', class_='card-body')\n",
    "        for lst in start.find_all('ul'):\n",
    "            for el in lst.find_all('li'):\n",
    "                us_data[ccode]['summary'] += el.text.replace('\\xa0', '') + ' '\n",
    "        us_data[ccode]['summary'] =  us_data[ccode]['summary'][:-1]\n",
    "        us_data[ccode]['summary'] = us_data[ccode]['summary'].replace('illnessfrom', 'illness from')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_data['GBR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN US SCRAPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_category = max([us_data[x]['category'] for x in us_data])\n",
    "max_category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, calculate restriction likelihood for future weeks\n",
    "### Update: US does not seem to have a numbers-based way of classifying countries in tiers. As we will not be able to make risk predictions considering external factors, we will simply keep existing categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_week = df_final[config['week_names'][-(config['lookahead']+1)]].values\n",
    "cat_info = {x: [] for x in range(max_category)}\n",
    "for itr in range(len(df_final)):\n",
    "    if not df_final.loc[itr, 'is_country']:\n",
    "        continue\n",
    "    ccode = df_final.loc[itr, 'CCODE']\n",
    "    if ccode in us_data:\n",
    "        cat_info[us_data[ccode]['category']-1].append(last_week[itr])\n",
    "# print(cat_info)\n",
    "# sns.distplot(cat_info[0], hist=True, norm_hist=True, kde=True)\n",
    "# sns.distplot(cat_info[1], hist=True, norm_hist=True)\n",
    "sns.distplot(cat_info[2], hist=True, norm_hist=True, kde=True, bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US Special: Repeat current categories with low confidence\n",
    "We can not predict categories from the data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_cat_info = {}\n",
    "df_cat = deepcopy(df_final[['CCODE']])\n",
    "df_cat['category'] = 2\n",
    "for itr in range(len(df_final)):\n",
    "    if not df_final.loc[itr, 'is_country']:\n",
    "        continue\n",
    "    ccode = df_final.loc[itr, 'CCODE']\n",
    "    if ccode in us_data:\n",
    "        df_cat.loc[itr, 'category'] = us_data[ccode]['category']-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate category-based predictions and confidence values\n",
    "\n",
    "# Get category prediction for every predicted scenario\n",
    "category_preds = np.stack([df_cat['category'].values] * config['lookahead'], 1)\n",
    "\n",
    "# Calculate confidence\n",
    "category_conf = category_preds.copy()\n",
    "category_conf[:] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add these extra bits of info to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_advice = deepcopy(df_final[['CCODE', 'Country', 'is_country']])\n",
    "df_advice['category'] = 2\n",
    "df_advice['advice_url'] = \"N/A\"\n",
    "df_advice['summary'] = 'N/A'\n",
    "\n",
    "for i, row in df_advice.iterrows():\n",
    "    # Get travel advice for countries and regions\n",
    "    if row['CCODE'] in us_data:\n",
    "        country_data = us_data[row['CCODE']]\n",
    "        df_advice.loc[i, 'category'] = country_data['category']-1\n",
    "        df_advice.loc[i, 'advice_url'] = country_data['advice_url']\n",
    "        df_advice.loc[i, 'summary'] = country_data['summary']\n",
    "    else:\n",
    "        if (row['Country'] == 'United States') and row['is_country']:\n",
    "            df_advice.loc[i, 'summary'] = ''\n",
    "        elif row['Country'] == 'United States':\n",
    "            df_advice.loc[i, 'summary'] = 'State-level travel advice not available.' \n",
    "        else:\n",
    "            df_advice.loc[i, 'summary'] = 'Travel advice not available.'\n",
    "        \n",
    "df_advice.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = {}\n",
    "fine_response = {}\n",
    "for itr in range(len(df_final)):\n",
    "    risk_vals = risk_preds[itr, :]\n",
    "    vals = category_preds[itr, :]\n",
    "    conf = category_conf[itr, :]\n",
    "    risk_vals = [int(x) for x in risk_vals]\n",
    "    vals = [int(x) for x in vals]\n",
    "    conf = [int(x) for x in conf]\n",
    "    country_response = {}\n",
    "    country_response[\"risk_preds\"] = risk_vals\n",
    "    if not df_final.loc[itr, 'is_country']:\n",
    "        country_response['lat'] = df_final.loc[itr, \"Lat\"]\n",
    "        country_response['lon'] = df_final.loc[itr, \"Lon\"]\n",
    "        country_response['country'] = df_final.loc[itr, \"Country\"]\n",
    "        country_response['ccode'] = df_final.loc[itr, 'CCODE']\n",
    "        country_response['graph_name'] = df_final.loc[itr, 'graph_name']\n",
    "        response[df_final.loc[itr, 'Name'] + ' ' + df_final.loc[itr, 'CCODE']] = country_response\n",
    "    else:\n",
    "        country_response[\"category\"] = int(df_advice.loc[itr, \"category\"])\n",
    "        country_response[\"category_preds\"] = vals\n",
    "        country_response[\"confidence_preds\"] = conf\n",
    "        country_response[\"summary\"] = df_advice.loc[itr, \"summary\"]\n",
    "        country_response[\"advice_url\"] = df_advice.loc[itr, \"advice_url\"]\n",
    "        response[df_final.loc[itr, 'CCODE']] = country_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{output_folder}/response-US.json', 'w') as json_file:\n",
    "    json.dump(response, json_file, indent='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to the cloud\n",
    "blob = bucket.blob(f\"preds/weekly/{cur_week_name}/response-US.json\")\n",
    "blob.upload_from_filename(f'{output_folder}/response-US.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = set(df_final[df_final['CCODE'] == '-1']['Country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2 = set(df_final[df_final['CCODE'] != '-1']['Country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure is empty\n",
    "c1 - c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
